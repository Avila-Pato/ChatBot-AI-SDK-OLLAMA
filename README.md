**Documentacion  Proyecto OllamaChat**
- ğŸ“ *Descripcion:c *Esto es una chat con IA local usando Ollama como backend y Next.js como frontend. Soporta streaming de respuestas y modelos ajustables.*


---

- ğŸ–¥ï¸ ConfiguraciÃ³n de Ollama
1. Descargar un modelo de ejemplo  
    - ollama pull phi3:mini

# Iniciar servidor con configuraciÃ³n optimizada
set OLLAMA_HOST=0.0.0.0:11435
set OLLAMA_NUM_GPU=0  # Desactivar GPU si hay poca VRAM
ollama serve

- ğŸš€ Estructura del Proyecto

```bash
/tuRuta
  /app
    /api
      /chat
        route.ts      
    page.tsx           
  /public
    ...               
```

